# **Risk Preference Trials**

<!-- Pre-catch Exclusion -->

<details>
  <summary>
  Details and complications with the risky-choice data
  </summary>

**Data is lumpy:**

- There is not a lot of distinction among the measured choice proportions.  All of the measurements can be categorized in to just 13 bins (see table below). For future studies, it might be worth adding additional testing trials to obtain a more continuous set of measurements. 

```{r echo = FALSE, message = FALSE, warning = FALSE, fig.width = 19, fig.height = 7}
source('r-scripts/risky_trials_analysis.R')

bins <- data.frame(table(subject_res$cp))
bins$Var1 <- as.numeric(as.character(bins$Var1))
bins$Freq <- as.integer(bins$Freq)
colnames(bins) <- c('CP', 'Frequency')

kable(t(bins), 'html', digits = 2) %>%
  kable_classic(full_width = F, position = "center", html_font = "Arial")
```

**High-value risky choices are bimodal**

- For the high-value choice, most values are either 0 or 1 (see histograms below). This is actually a important aspect of the data that (probably?) hasn't been noticed/discussed in past extreme-outcome papers.  Looking at the means, the high-value choices appear to be around indifference ($\approx 0.46$), but the histograms show that this is an artifact of the aggregation.

- At the level of the individual, most participants either completely prefer the fixed alternative or completely prefer the risky alternative with some in between. Statistically, it's not clear what the best way to handle this fact given the between- and within- subject manipulations. In any case, it's probably worth pointing out to the readers that the mean proportion for the high-value choices is not really describing the individual data all that accurately. **i.e., The population is indifferent; however, the individuals are not (generally speaking).**

**Low-value choices are heavily skewed**

- By and large participants are only choosing the fixed option, creating very heavy skew.  Consequently, people are much more risk-averse in the low-value context than the means would suggest they are (compare the low value mean vs median choice proportions in the summary stat table).

**Classic ANOVA is probably inadequate**

- A conventional ANOVA does not model this data very well. Note the non-normal residuals in the Q-Q plot below (from the ANOVA run on the differences). The pre-registration document calls for three separate ANOVAs, which are provided below. However, given the residuals, it's probably not wise to rely on a conventional ANOVA methodology. i.e., one based around ordinary least-squares (OLS). Consequently, an ANOVA using generalized least-squares (GLS) fit by maximum likelihood has been applied.

- A perhaps(?) slightly better (i.e., more robust) alternative (that is still consistent with your pre-registration) would be to conduct three bootstrapped heteroscedastic one-way ANOVAs for trimmed means. Though, that analysis isn't shown here because it doesn't impact the conclusions and doesn't allow for planned contrasts. Further, trimming is a foreign concept for many that is likely to perturb reviewers unfamiliar with it - though, the averages for the low-value choices would probably be more accurate. 

- In addition to the three one-way ANOVAs, a 2x3 ANOVA using a linear mixed effects (LME) models fit by maximum likelihood has been run - because it seems like the more elegant way to approach this particular experimental design which has both between- and within- subjects factors.

- Having said all that, no matter what approach you take (OLS, GLS, trimmed-bootstraps, or LME), you will end up drawing the same conclusions. Though, the 2x3 LME approach seems the most elegant way of arriving at them and will probably read the best in the manuscript.

<br>

```{r, fig.width = 19, fig.height = 7}
# Histograms
hist <- ggplot(subject_res, aes(x = cp)) +
  geom_histogram(aes(fill = risky_choice),
    colour = "black",
    bins = 13
  ) +
  facet_grid(risky_choice ~ condition) +
  scale_x_continuous(breaks = seq(0, 1, 0.5)) +
  theme_custom() +
  theme(
    axis.text = element_text(colour = "black", size = 16),
    axis.title = element_text(colour = "black", size = 16),
    strip.text = element_text(
      size = 20, colour = "black",
      margin = margin(1, 0, 1, 0, "cm")
    ),
    panel.spacing = unit(1.25, "lines")
  )

# QQ plot of difference model
qq <- ggplot(mapping = aes(sample = diff_mod$residuals)) +
  stat_qq() +
  stat_qq_line() +
  xlab("Theoretical Quantiles") +
  ylab("Residuals") +
  theme_custom() +
  theme(
    axis.text = element_text(colour = "black", size = 16),
    axis.title = element_text(colour = "black", size = 16),
    strip.text = element_text(
      size = 14, colour = "black",
      margin = margin(1, 0, 1, 0, "cm")
    ),
    panel.spacing = unit(1.25, "lines")
  )

hist + plot_spacer() + qq + plot_layout(widths = c(5, 1, 5))
```

</details> 

<details>
  <summary>
  Summary statistics
  </summary>

<br>

<h3> Summary statistics for risky choice trials </h3>

```{r echo = FALSE, message = FALSE, warning = FALSE, fig.width = 5, fig.height = 4}

s <- subject_res %>%
  group_by(condition, risky_choice) %>%
  summarise(
    n = length(cp),
    mean = mean(cp),
    sd = sd(cp),
    median = median(cp),
    IQR = IQR(cp)
    )

kable(s, 'html', digits = 3) %>%
  kable_classic(full_width = T, position = "center", html_font = "Arial") %>%
  column_spec(c(4, 6), bold = T)
```

<br>

</details> 

<details>
  <summary>
  Plot - Barplot of means
  </summary>

Note: 

- Errorbars are 95% CIs.
- Bootstrapping produces no appreciable difference in the obtained means or errorbars.

<br>

```{r, fig.width = 15, fig.height = 15}
#source('R Scripts/risky_trials_analysis.R')
plts_risky
```

</details> 

<details>
  <summary>
  Plot - Violin plot
  </summary>
  
Given its general weirdness, it seems unlikely you would include this in the manuscript, but its a much more accurate depiction of your data than either of the barplots are.

How to read:

- The fatter the shape, the more dense the values are at that location on the y-axis.
- The point range (circle with vertical line) shows the mean and 95% CI.
- The thick horizontal line shows the median.
- The small transparent points are individual scores with some random horizontal noise added to prevent overplotting. Notice the data clumps into 13 spots.

```{r, fig.width = 12, fig.height = 5.6}
#source('R Scripts/risky_trials_analysis.R')
plt_risky_violin
```

</details> 

<details>
  <summary>
  Plot - EO difference by block
  </summary>
  
  <br>
  
```{r, fig.width = 12, fig.height = 5.6}
#source('R Scripts/risky_trials_analysis.R')
plt_risky_diff_blks
```  
  
</details>

<details>
  <summary>
  Plot - High and low risk preference by block
  </summary>
  
  <br>
  
```{r, fig.width = 12, fig.height = 7}
#source('R Scripts/risky_trials_analysis.R')
plt_risky_std_blks
```  
  
</details>

<!-- One-Way ANOVA on Difference (High - Low) -->

<details>
  <summary>
  One-way ANOVA on difference / EO effect (high - low)
  </summary>
  
<h3>ANOVA Table</h3>

```{r echo = FALSE, message = FALSE, warning = FALSE}
kable(anova_diff, 'html', digits = 3) %>%
  kable_classic(full_width = T, html_font = "Arial")
```

Note: 

- ANOVA was run using generalized least-squares fit by maximum likelihood.
- $R^2_N$ is Nagelkerke's (also known as Cragg and Uhler's) pseudo $R^2$ [@Nagelkerke]. It isn't interpreted in exactly the same way as a $R^2$ based on ordinary least-squares regression, but it is bounded by 0 and 1 and higher values indicate a better model fit.

<br>

<!-- Planned Contrasts -->

<h3>Planned contrasts</h3>

```{r echo = FALSE, message = FALSE, warning = FALSE}
kable(pc_diff, 'html', digits = 3) %>%
  kable_classic(full_width = T, html_font = "Arial")
```

Note:

- E1_v_E2NE = *Extreme First* vs. combined effect of *Extreme Last* and *No Extreme*
- E2_v_NE = *Extreme Last* vs. *No Extreme*

<br>

<!-- Pairwise Comparisons -->

<h3>Pairwise comparisons: Post-hoc comparisons for heteroscedastic means</h3>

```{r echo = FALSE, message = FALSE, warning = FALSE}
kable(ph_diff, 'html', digits = 3) %>%
  kable_classic(full_width = T, html_font = "Arial")
```

Note:

- $\hat{\Psi}$ is the difference between condition means.
- These post-hoc tests don't assume equal variance, but do assume normality. Trimming can easily be added to make it more robust to non-normality.
- The Hochberg correction [@hochberg1988] was applied to protect family-wise error rate, but others like Holm's or Benjamini & Hochberg's could be used instead.

<br>

<h3>Conclusions</h3> 

```{r echo = FALSE, message = FALSE, warning = FALSE}
f <- round(anova_diff$`F-value`[2], 2)
df <- round(anova_diff$numDF[2:3])
p <- round(anova_diff$`p-value`[2], 3)
R2 <- round(anova_diff$pseudo_R2[2], 2)

b = round(pc_diff$Value[c(2, 3)], 3)
df_2 = round(pc_diff$DF[c(2, 3)])
t = round(pc_diff$`t-value`[c(2, 3)], 2)
p_2 = round(pc_diff$`p-value`[c(2, 3)], 3)
r = round(pc_diff$r_effect[c(2, 3)], 3)
```


**Effect of condition:**

$F(`r df[1]`, `r df[2]`) = `r f`, p = `r p`, R^2_N = `r R2`$ 

- There is no significant effect.

<br>

**Planned comparisons:**

Extreme First vs. (Extreme Last & No Extreme)

$t(`r df_2[1]`) = `r t[1]`$, $p = `r p_2[1]`$, $r = `r r[1]`$

- The *Extreme First* condition is significantly different from the combination of the other two conditions. Note that the effect is small and may be of little practical value.

<br>

Extreme Last vs. No Extreme

$t(`r df_2[2]`) = `r t[2]`$, $p = `r p_2[2]`$, $r = `r r[2]`$

- The *Extreme Last* and *No Extreme* conditions do not significantly differ. 

<br>

</details>

<!-- One-Way ANOVA on High Value Choices -->

<details>
  <summary>
  One-way ANOVA on high-value choices
  </summary>
  
<h3>ANOVA table</h3>

```{r echo = FALSE, message = FALSE, warning = FALSE}
kable(anova_hv, 'html', digits = 4) %>%
  kable_classic(full_width = T, html_font = "Arial")
```

Note: 

- ANOVA was run using generalized least-squares fit by maximum likelihood.
- $R^2_N$ is Nagelkerke's (also known as Cragg and Uhler's) pseudo $R^2$ [@Nagelkerke]. It isn't interpreted in exactly the same way as a $R^2$ based on ordinary least-squares regression, but it is bounded by 0 and 1 and higher values indicate a better model fit.

<br>

<!-- Planned Contrasts -->

<h3>Planned contrasts</h3>

```{r echo = FALSE, message = FALSE, warning = FALSE}
kable(pc_hv, 'html', digits = 4) %>%
  kable_classic(full_width = T, html_font = "Arial")
```

Note:

- E1_v_E2NE = *Extreme First* vs. combined effect of *Extreme Last* and *No Extreme*
- E2_v_NE = *Extreme Last* vs. *No Extreme*

<br>

<!-- Pairwise Comparisons -->

<h3>Pairwise comparisons: Post-hoc comparisons for heteroscedastic means</h3>

```{r echo = FALSE, message = FALSE, warning = FALSE}
kable(ph_hv, 'html', digits = 3) %>%
  kable_classic(full_width = T, html_font = "Arial")
```

Note:

- $\hat{\Psi}$ is the difference between condition means.
- These post-hoc tests don't assume equal variance, but do assume normality. Trimming can easily be added to make it more robust.
- The Hochberg correction [@hochberg1988] was applied to protect family-wise error rate, but others like Holm's or Benjamini & Hochberg's could be used instead.

<br>

<h3>Conclusions</h3> 

```{r echo = FALSE, message = FALSE, warning = FALSE}
f <- round(anova_hv$`F-value`[2], 3)
df <- round(anova_hv$numDF[2:3])
p <- round(anova_hv$`p-value`[2], 3)
R2 <- round(anova_hv$pseudo_R2[2], 3)

b = round(pc_hv$Value[c(2, 3)], 3)
df_2 = round(pc_hv$DF[c(2, 3)])
t = round(pc_hv$`t-value`[c(2, 3)], 2)
p_2 = round(pc_hv$`p-value`[c(2, 3)], 3)
r = round(pc_hv$r_effect[c(2, 3)], 3)
```


**Effect of of condition:**

$F(`r df[1]`, `r df[2]`) = `r f`, p = `r p`, R^2_N = `r R2`$ 

- There is no significant effect.

<br>

**Planned comparisons:**

Extreme First vs. (Extreme Last & No Extreme)

$t(`r df_2[1]`) = `r t[1]`$, $p = `r p_2[1]`$, $r = `r r[1]`$

- The *Extreme First* condition is not significantly different from the combination of the other two conditions.

<br>

Extreme Last vs. No Extreme

$t(`r df_2[2]`) = `r t[2]`$, $p = `r p_2[2]`$, $r = `r r[2]`$

- The *Extreme Last* and *No Extreme* conditions do not significantly differ. 

<br>

</details>

<!-- One-Way ANOVA on Low Value Choices -->

<details>
  <summary>
  One-way ANOVA on low-value choices
  </summary>
  
<h3>ANOVA Table</h3>

```{r echo = FALSE, message = FALSE, warning = FALSE}
kable(anova_lv, 'html', digits = 3) %>%
  kable_classic(full_width = T, html_font = "Arial")
```

Note: 

- ANOVA was run using generalized least-squares fit by maximum likelihood.
- $R^2_N$ is Nagelkerke's (also known as Cragg and Uhler's) pseudo $R^2$ [@Nagelkerke]. It isn't interpreted in exactly the same way as a $R^2$ based on ordinary least-squares regression, but it is bounded by 0 and 1 and higher values indicate a better model fit.

<br>

<!-- Planned Contrasts -->

<h3>Planned contrasts</h3>

```{r echo = FALSE, message = FALSE, warning = FALSE}
kable(pc_lv, 'html', digits = 3) %>%
  kable_classic(full_width = T, html_font = "Arial")
```

Note:

- E1_v_E2NE = *Extreme First* vs. combined effect of *Extreme Last* and *No Extreme*
- E2_v_NE = *Extreme Last* vs. *No Extreme*

<br>

<!-- Pairwise Comparisons -->

<h3>Pairwise comparisons: Post-hoc comparisons for heteroscedastic means</h3>

```{r echo = FALSE, message = FALSE, warning = FALSE}
kable(ph_lv, 'html', digits = 3) %>%
  kable_classic(full_width = T, html_font = "Arial")
```

Note:

- $\hat{\Psi}$ is the difference between condition means.
- These post-hoc tests don't assume equal variance, but do assume normality. Trimming can easily be added to make it more robust.
- The Hochberg correction [@hochberg1988] was applied to protect family-wise error rate, but others like Holm's or Benjamini & Hochberg's could be used instead.

<br>

<h3>Conclusions</h3> 

```{r echo = FALSE, message = FALSE, warning = FALSE}
f <- round(anova_lv$`F-value`[2], 2)
df <- round(anova_lv$numDF[2:3])
p <- round(anova_lv$`p-value`[2], 3)
R2 <- round(anova_lv$pseudo_R2[2], 2)

b = round(pc_lv$Value[c(2, 3)], 3)
df_2 = round(pc_lv$DF[c(2, 3)])
t = round(pc_lv$`t-value`[c(2, 3)], 2)
p_2 = round(pc_lv$`p-value`[c(2, 3)], 3)
r = round(pc_lv$r_effect[c(2, 3)], 2)
```


**Effect of of condition:**

$F(`r df[1]`, `r df[2]`) = `r f`, p = `r p`, R^2_N = `r R2`$ 

- There is a significant effect.

<br>

**Planned Comparisons:**

Extreme First vs. (Extreme Last & No Extreme)

$t(`r df_2[1]`) = `r t[1]`$, $p = `r p_2[1]`$, $r = `r r[1]`$

- The *Extreme First* condition is significantly different from the combination of the other two conditions.

<br>

Extreme Last vs. No Extreme

$t(`r df_2[2]`) = `r t[2]`$, $p = `r p_2[2]`$, $r = `r r[2]`$

- The *Extreme Last* and *No Extreme* conditions do not significantly differ. 

<br>

</details>

<!-- 2x3 ANOVA using LME Models Fit by Maximum Likelihood -->

<details>
  <summary>
  2x3 ANOVA using linear mixed-effects models fit by maximum likelihood
  </summary>

<h3>ANOVA Table</h3>

```{r echo = FALSE, message = FALSE, warning = FALSE}
kable(aov_2x3[, -c(1)], 'html', digits = 3) %>%
  kable_classic(full_width = T, html_font = "Arial")
```

- base = intercept only (used as baseline and predicts overall mean ignoring other factors)
- cond_mod = effect of adding condition
- HL_mod = effect of adding context (i.e., High / Low choices)
- int_mod = interaction effect

<br>

<h3>Planned contrasts</h3>

```{r echo = FALSE, message = FALSE, warning = FALSE}
kable(round(pc, 3), 'html', digits = 3) %>%
  kable_classic(full_width = T, html_font = "Arial")
```

- E1_v_E2NE = *Extreme First* vs. combined effect of *Extreme Last* and *No Extreme*
- E2_v_NE = *Extreme Last* vs. *No Extreme*
- risky_choice1 = *High vs Low* (same test as row 3 in the ANOVA table)

<br>

<h3>Conclusions</h3> 

```{r echo = FALSE, message = FALSE, warning = FALSE}
df = aov_2x3$df[2:4] - aov_2x3$df[1:3]
chi = round(aov_2x3$L.Ratio[2:4], 2)
p = round(aov_2x3$`p-value`[2:4], 3)
R2 = round(aov_2x3$pseudo_R2[2:4], 2)
BF = format(round(aov_2x3$BF_10[2:4], 2), scientific = FALSE)

b = round(pc$Value[c(2, 3, 5, 6)], 3)
df_2 = round(pc$DF[c(2, 3, 5, 6)])
t = round(pc$`t-value`[c(2, 3, 5, 6)], 2)
p_2 = round(pc$`p-value`[c(2, 3, 5, 6)], 3)
r = round(pc$r_effect[c(2, 3, 5, 6)], 3)
```

**Main effect of of condition:**

$\chi^2 (`r df[1]`) = `r chi[1]`, p = `r p[1]`, R^2_N = `r R2[1]`, BF_{10} = `r BF[1]`$

- There is no main effect of condition.

<br>

**Main effect of of context (high vs low):**

$\chi^2 (`r df[2]`) = `r chi[2]`, p = `r p[2]`, R^2_N = `r R2[2]`, BF_{10} = `r BF[2]`$

- There is a main effect of context (high-value choices vs. low-value).

<br>

**Interaction:**

$\chi^2 (`r df[3]`) = `r chi[3]`, p = `r p[3]`, R^2_N = `r R2[3]`, BF_{10} = `r BF[3]`$

- There is no overall interaction.

<br>

**Planned comparisons:**

Given the lack of interaction and main effect of condition, it's debatable whether the extra step of interpreting the planned comparisons should be taken. However, since the planned comparisons test more refined *a priori* predictions, it seems like they might be more important than the main effects, which lack nuance.

<br>

Extreme First vs. (Extreme Last & No Extreme)

$t(`r df_2[1]`) = `r t[1]`$, $p = `r p_2[1]`$, $r = `r r[1]`$

- The *Extreme First* condition is significantly different from the combination of the other two conditions. Note that the effect is small and may be of little practical value.

<br>

Extreme Last vs. No Extreme

$t(`r df_2[2]`) = `r t[2]`$, $p = `r p_2[2]`$, $r = `r r[2]`$

- The *Extreme Last* and *No Extreme* conditions do not significantly differ. 

<br>

Extreme First vs. (Extreme Last & No Extreme) | Context (i.e., High vs. Low)

$t(`r df_2[3]`) = `r t[3]`$, $p = `r p_2[3]`$, $r = `r r[3]`$

- The first planned comparison significantly differs depending on context (i.e., the Extreme First condition only differs from the other two conditions for the low-value choices). Note that the effect is small and may be of little practical value.

<br>

Extreme Last vs. No Extreme | Context (i.e., High vs. Low)

$t(`r df_2[4]`) = `r t[4]`$, $p = `r p_2[4]`$, $r = `r r[4]`$

- The second planned comparison does not significantly differ depending on context (i.e., no interaction with context).

<br>


**Note:** 

- $p = 0$ should be reported as $p < .001$

- $R^2_N$ is Nagelkerke's (also known as Cragg and Uhler's) pseudo $R^2$ [@Nagelkerke]. It isn't interpreted in exactly the same way as a $R^2$ based on ordinary least-squares regression, but it is bounded by 0 and 1 and higher values indicate a better model fit.

- Bayes Factors are reported as Inverse Bayes Factors, this means larger values
indicate greater support for the effect. Anything less than 1 should be considered as contributing no evidence.

- If $BF_{10} > 150$, report that instead specifying the entire value.

</details>
























